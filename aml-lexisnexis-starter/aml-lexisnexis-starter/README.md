
# AML Learning Project â€” LexisNexis + Transaction Anomaly Detection (Phase 1)

**Goal:** Learn ML/AI/NLP by building a realistic **AML anomaly detection** pipeline that links **LexisNexis (KYC/enrichment)** data with a **transaction dataset**, and flags unusual behavior with **explainable** outputs.

## ğŸ“¦ Project Structure
```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # place source CSVs here (transactions.csv, lexisnexis.csv)
â”‚   â”œâ”€â”€ processed/      # clean, joined, feature-rich datasets
â”‚   â””â”€â”€ external/       # any third-party lookups (country risk lists, MCC codes, etc.)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ SETUP.md        # how to run the project
â”œâ”€â”€ notebooks/          # exploratory notebooks (01, 02, ...)
â”œâ”€â”€ src/aml/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_ingest.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ isolation_forest.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_smoke.py
â”œâ”€â”€ README.md
â”œâ”€â”€ ROADMAP.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## ğŸ§© Phase 1 â€” What we will build
1. **Data linking**: Join transactions with LexisNexis KYC/enrichment by customer/account ID.
2. **Features**: velocity, peer-group deviation, geo/country, channel, counterparty risk, risk flags from LexisNexis.
3. **Model**: Unsupervised baseline â€” Isolation Forest (configurable).
4. **Explainability**: Top contributing features per flag (simple SHAP summary or rule contributions).
5. **Outputs**: CSV of flagged transactions with scores, and a simple notebook dashboard.

> **Note:** Use synthetic/sanitized data for learning. No real PII should be committed.

## ğŸ“‚ Expected Input Files (CSV)
- `data/raw/transactions.csv`:  
  `txn_id, customer_id, account_id, datetime, amount, currency, channel, counterparty_id, counterparty_country, mcc, description`
- `data/raw/lexisnexis.csv`:  
  `customer_id, customer_name, dob, nationality, pep_flag, sanctions_flag, adverse_media_score, risk_rating, kyc_last_review_date`

(Columns are examples â€” adjust in `src/aml/config.py` if your dataset differs.)

## â–¶ï¸ Quickstart
```bash
python -m venv .venv
source .venv/bin/activate  # on Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Put your CSVs in data/raw/, then run a smoke test:
python -c "from src.aml.data_ingest import validate_inputs; print(validate_inputs())"
```

## ğŸ§ª First Notebook Plan
- `notebooks/01_data_link_and_eda.ipynb`: join, sanity checks, basic EDA.
- `notebooks/02_baseline_isolation_forest.ipynb`: train, score, export flagged csv.
- `notebooks/03_explainability.ipynb`: SHAP (or simpler feature deltas), flag reasons.
- `notebooks/04_reporting.ipynb`: top-10 customers/transactions, simple visuals.

## ğŸ§­ Next Steps (Phase 1 â†’ 1.5 â†’ 2)
See **ROADMAP.md** for milestones, issue bundles, and stretch goals (graph, NLP, case pages).
